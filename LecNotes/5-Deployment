# [Deployment](https://fullstackdeeplearning.com/course/2022/lecture-5-deployment/)


# Protype Deployment
## Tools
    - HuggingFace Spaces
    - Gradio
    - Streamlit
## Best Practices
    1. Have a basic UI
        - Easier for feedback with friends and coworkers
        - Gradio and Streamlit are your friends
    2. Put it behind a web URL
        - Easier to share
        - Cloud versions of streamlit and huggingface are helpful here
        - Don't stress too much

## Where will prototype deployment fail? 
    1. Limited frontend flexibility
    2. Don't scale well to many concurrent requests: the model becomes the bottleneck

<br/>

# Web Deployment Architecture
## 1. Model-in-service
    Pros: 
        - Re-uses your existingn infrastructure
    Cons: 
        - Web server may be written in a different language 
        - Models may change more frequently than server code
        - Large models can eat into the resources for your web server
        - Server hardware not optimized for your model (no GPUs)
        - Model and server may scale differently
## 2. Model-in-database
    Periodically run your model on new data and save the results in a database
    Works if the universe of inputs is relatively small
        - Recommender systems
        - Marketing automation
    Data processing/workflow tools work well here 
        - dagster, Apache Airflow, MetaFlow, Prefect, AWS Step Functions
    Pros: 
        - Simple to implement
        - Scales easily
        - Used in production by large-scale production systems for years
        - Fast to retrieve the prediction
    Cons: 
        - Doesn't scale to complex input types (user-specified queries)
        - Users don't get the most up-to-date predictions
        - Models frequently become "stale", which can be hard to detect
## 3. Model-as-service
    - Run your model on its own web server
    - The backend (client itself) interacts with the model by making requests to the 
    model service and receiving responses back
    - Pros: 
        - Dependability - model bugs less likely to crash web app
        - Scalability - choose optimal hardware for the model and scale it appropriately
        - Flexibility - easily reuse a model across multiple apps
    - Cons: 
        - Can add latency
        - Adds infrastructural complexity
        - Now you have to run a model service...

<br/>

# Build a Model Service: The Basics

## 1. REST APIs
    - Serving predictions in response to canonically-formatted HTTP requests
    - There are alternatives like GRPC (tensorflow serving) and GraphQL (not terribly relevant)
    - Currently no standard for sending requests

## 2. Dependency Management
    - Model predictions depend on code, model weights, and dependencies
        - Everything needs to be present
    - Dependencies are notoriously difficult to manage
        - Difficult to keep consistency between development code and deployed model
        - Hard to update across all places
        - Even changing a tensorflow version can change your model

## 3. Managing Dependencies
### Constrain the dependencies of you model
    - ONNX (Open Neural Network Exchange) is the leader here
        - Promise: define network in any language, run it consistently anywhere
        - Doesn't really work as promised
    - ONNX doesn't deal with non-library code
### Containers
    - Docker vs. Virtual Machines (VMs)
        - Docker removes need to package OS 
        - Docker is lightweight (VMs are heavy)
    - Because Docker is lightweight...
        - Web app might have 4 different containers
            - Web server, Database, Job queue, Worker, etc.
    - Creating Docker container
        - run a Dockerfile
        - You can build containers on your laptop 
        - Can build, store, and pull Docker container stored on some servers
    - Docker components
        1. Client: run locally
        2. DOCKER_HOST
        3. Registry: stores containers
    - Docker is pretty industry standard
    - OpenSource packages to make Docker easier to learn
        - Cog, BentoML, Truss

## 4. Optimization
### GPUs
    - Pros: 
        - Same hardware you trained on 
        - Limit of model size, batch size tuning, usually higher throughput
    - Cons: 
        - More complex to set up
        - Almost always more expensive
    - Just because your model was trained on a GPU doesn't mean you needs to serve it on GPU
### Concurrency
    - Multiple copies of the model running on different CPUs or cores
    - Be careful about thread tuning
### Model Distillation
    - Trainer a smaller model to imitate your larger one
    - See video for blog post
    - This is actually rarely used in practice
### Quantization
    - Execute some or all of the operations in your model with a smaller numerical representation than floats
    - Some tradeoffs with accuracy
    - PyTorch and Tensorflow LIte have quantization built-in
    - Can also run quantization-aware training --> often produces higher accuracy
    - Optimum (HuggingFace)
### Caching
    - For some ML models, some inputs are more common than others
    - Instead of always calling the model, first check the cache
    - You can get very fancy
    - Basic: use functool in Python
### Batching
    - ML models often achieve higher throughput when doing prediction in parallel, especially in a GPU
    - Gather predictions until you have a batch, run predication, return to user
    - Batch size needs to be tuned 
    - You need to have a way to shortcut the process if latency becomes too long
    - Probably don't want to implement this yourself
### Sharing the GPU
    - You model may not take up all of the GPU memory with your inference batch size
    - Why not run multiple models on the same GPU? 
    - You'll want to use a model serving solution that supports this out of the box
    - Model serving libraries
        - TensorFlow Serving, PyTorch, Nvidia, Ray Serve

## 5. Horizontal Scaling
    - If you have too much traffic for a single machine --> split to multiple machines
    - Have multiple copies of your service and split traffic using a load balancer

### Container Orchestration
    - You need a different tool (outside of Docker) to ensure all machines are running the same code
    - Kubernetes
        - Very interesting topic --> worth reading about for distributed and concurrent computing

### Serverless Functions (start here)
    - App code and dependencies are pacakged into .zip files or docker containers with a single entry point function
    - AWS Lambda (Google Cloud Functions, or Azure Functions) manages everything else  
        - instant scaling to 10,000+ reqeusts per second
        - load balancing
    - Only pay for the time the server is actually being used
    - Cons: 
        - Limited size of deployment package
        - Cold start: takes awhile to start up after first request
        - Can be challending to build pipelines of models
        - Little to no state management
        - Limited deployment tooling
        - CPU-only, limited execution time (this problem should disappear quickly)

### Model Rolouts
    - Serving: turn a model into something that can respond to requests
    - Rollouts: how you manage and update these services
    - You want to be able to 
        - Rollout graduallly
        - Roll back instantly
        - Split traffic between versions
        - Deplooy pipelines of models
    - This is a challenging infrastructure problem (beyond scope of this lecture)

### Managed Options
    1. Cloud Providers
        - Google AI
        - Amazon SageMaker
        - Azure Machine Learning
    2. End-to-end ML Platforms
        - databricks
        - DataRobot
    3. Startups
        - Cortex
        - BentoML
        - Banana
        - Seldom
        - Neural Magic
    - Amazon SageMaker

### Takeaways
    - You usually don't need GPUs
    - If using CPU inference, can get away with scaling by launching more servers, or going serverless
    - Serverless makes sense if you can get away with CPUs and traffic is spikyor low-volume
    - Sagemaker perfectly good start (if using AWS)

# Edge
    - Sometimes it's obvious
        - No reliable internet connection
        - Very strict data security / privacy requirements
    - Otherwise
        - Accuracy and latency both affect end-user experience
        - Latency included the network roundtrip and model prediction time
        - Once you've exhausted optins for reduction model prediction time, consider edge
    - Send model weights to client device --> interact directly
    Pros: 
        - Lowest-latency
        - Does not require an internet connection
        - Data security - doesn't need to leave the user's device 
        - Scale comes "for free"
    Cons: 
        - Often limited hardware resources available
        - Embeded and mobile frameworks ar less full featured than tensorflow/ pytorch
        - Difficult to update models
        - Difficult to monitor and debug when thing go wrong

# Frameworks
More edge stuff, doesn't apply to us right now


# Review
- Deploy early, deply often 
- Keep it simple, add complexity later
    - Build a prototype
    - Separate your model and UI
    Learn the tricks to scale
    - Consider moving your model to the edge when you *really* need it
    





        

